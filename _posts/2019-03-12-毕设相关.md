---
layout:     post
title:      毕设-深度学习
subtitle:   毕设内容
date:       2019-03-14
author:     Hxd
header-img: img/city.jpg
catalog: true
tags: 
  - 毕业设计
  - 深度学习
  - 机器学习
  - Python
---

# 知识储备

## Python

### 中文编码

在代码前面添加

```python
# -*- coding: UTF-8 -*-

...
```

### 变量

**原始字符串**

```python
str = r'hello word!'
```

## 数学基础

### 线性代数

**标量（scalar）：** 一个单独的数，比如速率。

**向量（vector）：** 一列数。 x = [1， 2， 3]。

**矩阵（matrix）：** 二维数组。
```
    i,j   i行j列
  ┌A1,1 A1,2┐
  └A2,1 A2,2┘
```

**张量（tensor）：** 坐标超过二维的数组。A(i,j,k)


### 高数

### 概率论



# 机器学习笔记
[![12.jpg](https://i.loli.net/2019/03/19/5c910833dea8e.jpg)](https://i.loli.net/2019/03/19/5c910833dea8e.jpg)

# 深度学习笔记

[![1553319764(1).jpg](https://i.loli.net/2019/03/24/5c96e47b49340.jpg)](https://i.loli.net/2019/03/24/5c96e47b49340.jpg)
[![1553319505(1).jpg](https://i.loli.net/2019/03/24/5c96e47b50653.jpg)](https://i.loli.net/2019/03/24/5c96e47b50653.jpg)


[![1553393904(1).jpg](https://i.loli.net/2019/03/24/5c96e93c43d1e.jpg)](https://i.loli.net/2019/03/24/5c96e93c43d1e.jpg)
[![1553397973(1).jpg](https://i.loli.net/2019/03/24/5c96f8e7337e4.jpg)](https://i.loli.net/2019/03/24/5c96f8e7337e4.jpg)[![1553393904(1).jpg](https://i.loli.net/2019/03/24/5c96e93c43d1e.jpg)](https://i.loli.net/2019/03/24/5c96e93c43d1e.jpg)
[![1553397973(1).jpg](https://i.loli.net/2019/03/24/5c96f8e7337e4.jpg)](https://i.loli.net/2019/03/24/5c96f8e7337e4.jpg)

# 毕设题目

一种基于Faster R-CNN架构的时序动作定位算法

参考内容：

- http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html

参考论文：

- Multi-region two-stream R-CNN for action detection
- Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
- Rethinking the Faster R-CNN Architecture for Temporal Action Localization
- Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs

论文相对应代码：

- https://github.com/pengxj/action-faster-rcnn

后续改进：增加在每一帧中事件具体位置的定位

## 论文翻译：

### Multi-region two-stream R-CNN for action detection

### 多区域双流 R-CNN 动作检测

**摘要：** 我们提出了一种用于现实视频动作检测的多区域双流R-CNN模型。我们以基于Faster R-CNN为基础的帧级动作检测的开始，做出了三个贡献：1.我们的实验表明，Motion Region Proposal网络产生高质量的prosposal，与Appearance Region Proposal网络是互补的；2.多帧叠加光流对帧级动作检测有明显的改善作用；3.我们在Faster R-CNN模型中嵌入了一个多区域方案，增加了身体部位的补充信息。我们将帧级检测与Viterbi算法联系起来，并使用最大子阵方法暂时定位一个动作。UCF-Sports、J-HMDB和UCF 101的实验结果检测数据集表明，我们的方法在frame-mAP和video-mAP上都有显著的优势，超过了最新的技术水平。

**1 介绍：** 视频中的动作识别有很多实际应用，比如监视，人机交互和基于内容的检索等。大多数研究工作都集中在行动分类上，其中一个类别标签被分配给整个视频。然而，给定视频流，动作发生在精确的时空范围。行动检测旨在确定这些位置，最近引起了越来越多的关注。由于大的类内变化，背景杂乱，特别是大的时空搜索空间，这是一个具有挑战性的问题。

以前的一些工作只针对时间定位，即它们只提供了一个动作的开始和结束时间。利用时间滑动窗口和密集轨迹特征获得最先进的结果。对于时空检测，最近的一些工作将二维目标检测模型扩展到三维模型。例如Tian等人将2D可变形部分模型扩展到3D可变形部分模型，Wang等人将poselets扩展为动态的poselet模型。最近的工作首先通过使用卷积神经网络（CNN）功能检测帧级别的动作，，然后链接它们或跟踪某些选定的检测以获得视频动作判断。由于CNN在目标检测方面的优异性能，这些基于帧级的方法实现了最先进的性能。这表明帧级动作检测的质量直接影响到视频中动作检测的质量。

因此，如何改进帧级动作检测是一个关键问题。Weinzaepfel等人通过使用更好的建议算法来改进帧级动作检测，比如EdgeBoxes。 Gkioxari等人通过添加上下文特征，增强基于R-CNN的静态图像中的动作检测。实际上，这两种方法表明了帧级检测的两个重要问题：1.高质量的proposal有助于CNNs精确地提取行动表示。2.动作表示对于检测至关重要。

本文主要研究基于帧级的动作检测方法，并旨在推进有关这两个关键方面的最新技术：框架级行动建议和行动表示。

`Frame-level action proposal` 基于区域proposal的目标检测的瓶颈之一是这些proposal的精确定位。为了解决行为检测中的这个问题，我们首先对rgb数据的帧级动作定位的三种proposal方法进行了评估：选择性搜索(SS)，EdgeBoxes (EB)，区域proposal网络(RPN)。我们表明RPN方法对外观信息的结果始终比其他方法具有更高的IoU评分。此外，我们将appearance RPN扩展到在视觉流数据上训练的motion RPN。我们观察到，motion RPN获得了高质量的proposal，这是对appearance RPN的补充。

`Action representation` 动作表示对于良好的性能至关重要。在这里，我们提出了一种基于双流CNN的改进的动作表示法，用于动作分类和多区域CNN。首先，我们将多帧视觉流叠加到Faster R-CNN模型，这大大改善了motionR-CNN。其次，我们选择了motion R-CNN的多主体区域（即上体、下体和边界区域），提高了基于帧的动作检测的性能；

总之，本文介绍了一种基于UCF-Sports、J-HMDB和UCF 101数据集的多区域双流R-cnn动作检测模型。我们的贡献如下：1.我们引入了一个motion RPN，它产生高质量的proposal，并补充了appearance RPN。2.我们表明，叠加视觉流显着地改善了帧级检测。3.我们在Faster R-CNN模型中嵌入了一个多区域方案，从而提高了结果性能。

本文的剩余部分如下：在第2节中，我们回顾了有关行动识别和区域CNNs的相关工作。我们在第3节中引入了motion RPN和叠加视觉流的双流RCNN。第4节讲多区域R-CNN。第五节讲时序连接和定位。第六节给出实践结果。

**2 相关工作** 

摘要近年来，动作识别和卷积神经网络(CNNs)得到了广泛的研究。本节仅介绍与本方法直接相关的方法。

`Action classification and detection动作分类与检测` 对于动作分类，大多数方法集中于如何表示整个视频。流行的视频表示是bag-of-visual-words (BoW)和它的变体，它综合了本地视频特征，cnn表示和慢速特征表示。Wang等人使用Fisher向量和密集轨迹与运动补偿。Peng将这种方法与叠加的Fisher向量结合起来。Sim等人设计了基于RGB数据和视觉流的双流CNN。Kar等人方明了几种基于appearance RPN的随时间融合信息方法。wang沿密集轨迹提取双流CNN。

对于动作检测，使用局部特征来表示动作，并依靠滑动窗口方案进行时间或时空定位。Rodriguez等人进行全局模板匹配。Tran等人使用BoW表示并实现最优的时空路径以进行动作检测。Tian等人将二维可变形部分模型扩展到三维空间-时间体积，以进行动作定位。Wang等人应用动态粒子和连续骨架模型联合检测动作和姿势。

`Region CNN for detection.Region CNN检测` 区域CNN(R-CNN)在静态图像的目标检测方面取得了显著的改进。该方法首先使用selective search提取区域proposal，并将其重新标出到固定的大小，然后使用标准CNN网络来训练和提取特征。这些特征随后被输入有着hard negative mining的SVM分类器和边框回归器。SPP-net通过空间金字塔池策略消除了固定输入大小的限制，从而对其进行了改进。Fast R-CNN通过引入ROI池方案和训练分类器以及边框回归器来加速R-CNN。Faster R-CNN进一步加速了快速的R-CNN，用RPN代替了selective search proposal方法。 Spyros等人增加了多区域和分段感知的CNN功能，以使R-CNN表现更有区别。

在R-cnn、Gkioxari和Malik的启发下，利用selective search对RGB帧的方法提取proposal，然后将原始R-cnn应用于每帧rgb和视觉流数据，用于帧级动作检测。最后由Viterbi算法链接检测生成动作。Wei用EdgeBox代替selective search方法进行proposal提取，并对选定的帧级检测进行跟踪。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nhkq1etwj20mq09njvm.jpg)

我们的工作与上述方法有四个不同之处：1.我们利用RPN从RGB和视觉流数据中产生丰富的proposal；2.利用叠加光流增强了R-CNN运动的分辨能力；3.我们通过在Faster R-CNN中嵌入一个多区域方案来进一步提高性能；4.我们建立了一个端到端的多区域双流cnn模型，用于帧级动作检测。

**3 End-to-end two-stream faster R-CNN端到端的双流faster r-cnn模型** 

图一（即上图）展示了我们的双流Faster R-CNN(TS R-CNN)方法。叠加光流对基于cnn的动作分类是有效的。我们相信，基于R-CNN的行动检测也会出现这种情况。我们的TS R-CNN以RGB帧ft和【几个为帧ft及其相邻帧提取的】光流图作为输入(我们在时间t之前取一半帧，在时间t之后取其中一半)。然后，网络用几个卷积和Max Pooling Layer来处理它们，在appearance和motion流中独立地处理它们。对于每个流，最后卷积层被输入appearance或motion RPN和ROI层。这里我们介绍了一个ROI融合层，它融合了appearance RPN和motion RPN的proposal。appearance RPN和motion RPN 的RoI pooling层都采用了所有的RoI，使用H×W网格为每个层执行Max-pooling操作。对于每个流，这些固定长度的特征向量随后被输入到一系列完全连接的层中，这些层最终分支到一个Softmax层和一个边框回归器中。两个流的最终检测结果可以通过几种方法组合，这些方法将在第6.3节中进行评估。最优性能是通过简单地结合Softmax分数来获得的。

`Training and testing` 我们分别训练两流Faster R-CNN。对于这两个流，我们在ImageNet数据集上对vgg-16模型进行了微调。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1ni8ixsmhj20nn0dzjv9.jpg)

一帧光流数据通过叠加x分量、y分量和流的大小被转换成3通道图像。在多个光流图的情况下，其中输入通道编号与VGG-16网络不同，我们只是重复第一层的VGG-16滤波器多次。我们使用中间框的地面真相包围框进行训练。为了进行测试，我们首先通过添加ROI融合层将所学的外观和运动R-CNN模型组合成一个模型，参见图1。然后，我们将帧流对放到端的模型中，并将来自这两个流的Softmax分数平均作为最终的动作区域检测分数。边界框回归器应用于每个流的相应ROI(参见图1中的红色和蓝色实心条)。这些框的级联是最终的检测结果。

`Evaluation of our action proposals评估我们的action proposal` 为了显示我们的运动RPN(RPN-m)的质量，我们将它与其他几种建议方法进行了比较。图2比较了以下所描述的不同建议方法的回顾性交叉重叠(IOU)。选择性搜索(SS)通过使用具有颜色、纹理和框大小特征的自下而上分组方案来生成区域。我们保留默认设置并获得2k proposal。EdgeBox(EB)是根据这样的观察得出的，即边框中全部包含的等高线数是客观性的指示。同样，我们使用默认设置并获得256 proposals。RPN方法首先为每个像素生成多个具有多尺度和比例的锚盒，然后用所学的特征对它们进行评分和回归。对于训练RPN，对于那些有高IOU与ground-truth boxes重叠的锚，获得了积极的客观标志。作为比较，我们保留RPN 300 proposals，并使用固定的最小边沿为600像素的一个比例尺。。我们还将rpn方法推广到光流，并报告了单流和叠加流的结果。图2显示了RPN-一个方法始终优于SS和EB，也就是说，当使用RGB帧时，它获得了最好的结果。有趣的是，在UCFSports上，它得到了25%的完美检测(IOU=1)(也就是说，recall=0.25表示IOU=1)。为了与SS和EB进行公平的比较(这两种方法都是非调优的操作数据集)，我们将在ImageNet上预训练的RPN的结果显示为图2中的RPN-ImageNet。在这两个数据集中，它也始终优于SS和EB。此外，单帧光流的motion RPN也提供了很好的action proposal。他们比RPN-a在UCFSports上更好，而在J-HMDB上更糟糕。这可以解释为与J-HMDB相比，在UCF-Sports上发生的更显著的运动，它包含了一些日常活动，没有明显的运动，例如“刷毛”、“倒”和“wave”。5层流(RPN-M5)和10层流(RPN-m10)使召回率增加。一种可能的解释是，叠加光流使得表示更具有判断力，但是由于时间盒的非对齐，对更多帧存在饱和。结合外观和运动两方面的方案，RPN取得了最佳的性能，并以显著优势优于SS和EB。

**4 Multi-region two-stream faster R-CNN多区域双流Faster R-CNN** 

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1niypn1lpj20mp0cnq90.jpg)

图3显示了多区域两流更快的R-CNN(MR-TS R-CNN)体系结构。它建立在双流Faster R-CNN之上，在RPN和ROI池层之间嵌入了一个多区域生成层。给出来自外观RPN和运动RPN的proposal，多区域层为每个RPN方案生成4个ROI。我们在下面描述了与动作表示相关的四种类型的区域。

original regions是origin RPN proposals。沿该信道的网络被引导以捕获整个动作区域。这个网络和TS R-CNN完全一样。边框回归器仅适用于此通道。“上半区”和“下半区”是RPN proposal的上半部分和下半部分，参见图3中多区域层中的第二行和第三行。在[18]中，我们只使用上/下半区域，而不是在[18]中用于物体的左/右/上/下半部区域，因为在动作视频中，主体的垂直结构主要是对称的。基于这些部分的网络不仅具有强大的w.r.t遮挡，而且对于身体部位特征占主导地位的动作类别也更具辨别力。例如，“高尔夫”和“挥杆棒球”只在上半部分更容易识别，而“爬楼梯”和“踢球”则只在下半区识别。

“边疆”区域是围绕origin proposal的长方形环。给出了RPN方案，通过将提案缩放为0.8倍，生成边界区域的内盒，并将外盒缩放为1.5倍，从而生成边界区域的内盒。对于外观流，沿着该通道的网络将联合捕获人和附近对象的外观边界，这可能有助于动作识别。对于运动流，该信道很有可能聚焦于运动边界区域，这对于手工制作的特性非常有用[2]。

`Training` 原始区域的两流网络是从上一节介绍的网络中复制而来的。为了训练其他地区的两流网络，我们分别对原区域的网络进行微调。特别是，我们只调整完全连接的层，并修复所有卷积层以及RPN，以确保所有区域网络共享相同的方案。对于“边界”区域两流网络，我们引入了一个掩码支持的ROI池层，它将内部盒内的激活设置为零，类似于[40，18]。在对区域网络进行培训之后，我们通过进一步培训另一个基于多区域两流网络的Softmax层来组合它们，参见图3。请注意，多区域R-CNN共享所有Conv层，因此测试期间的计算成本只增加1.8倍。

**5 Linking and temporal localization连接和瞬时定位** 

在上述方法的基础上，得到了帧级动作检测。为了实现视频级检测，我们采用了与相似的链接和基于最大子阵算法的时间定位。给定连续帧t和t1的两个区域rt和rt 1，我们定义了一个动作类c的链接得分。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1njox7s1cj20l401tdfq.jpg)

其中sc(Ri)是区域Ri的类得分，ov是两个区域的交叉超并重叠，β是标量。ψ(Ov)是由ψ(Ov)=1定义的阈值函数，如果ov大于否则，τ，ψ(Ov)=0。我们通过实验观察到，我们的链接得分比[8]中的好，并且由于额外的重叠约束，我们的连接更健壮。在计算完所有连接分数之后对于一个动作，我们用维特比算法迭代地确定最优路径，从而得到视频级的动作检测。最后通过![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1njrgei6pj2079011744.jpg)对视频级动作检测R=[R1，R2，.，RT]。为了确定视频轨道中动作检测的时间范围，可以应用具有多个时间尺度和步长的滑动窗口方法作为[9]。这里我们依赖于一种有效的最大子阵方法。

给定一个视频级检测R，我们的目标是从帧s到帧e找到满足以下目标的检测，

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1njtbbsstj20jk02bt8r.jpg)

其中L(s，e)是航迹长度，LC是c级在训练集上的平均持续时间。我们建议用三个步骤近似地解决这个问题：(1)从所有帧级动作分数中减去视频长度动作分数sc(R)，(2)求出减法的最大子阵。ED阵列采用Kadane算法[41]，(3)将最优范围扩展或缩短到LC。我们的解决方案只搜索一次轨道。对于每一个视频长度动作检测，我们只保留最佳的时空检测范围。注意，三步启发式是对方程(2)的近似，步骤(3)设置从步骤(2)到平均长度的最优管的长度，以避免退化解。

**6 Experiments实验**

在本节中，我们首先介绍了数据集和评估度量的细节，并描述了实现细节。然后我们对我们的方法进行了全面的评估，并将其与现有的技术进行比较。

`6.1 Datasets and evaluation metrics数据集和评价指标` 

在我们的实验中，我们评估了三个数据集的动作检测：UCF-Sports，J-HMDB和UCF-101.我们将在下面简要回顾它们，并介绍用于评估的指标。

UCF-Sports[31]包含10个不同体育课的150个短片。视频只截取了动作部分作，并为所有帧提供边界框注释。我们使用标准的训练[31]中定义的测试拆分。

J-HMDB[20]包含928个视频，用于21种不同的动作，如梳头、挥杆棒球或跳跃。视频剪辑仅限于动作的持续时间。每个剪辑包含15到40之间。框架。所有帧都有人体轮廓标注。地面-真相包围盒是从剪影推断出来的。有三个训练/测试分裂和评价平均结果在第四。雷分裂了。

UCF-101[42]致力于行动分类，有13000多个视频和101个类别。对于24个标签和3207个视频的子集，对动作的时空范围进行注释。所有的实验都是在第一个部分上进行的。与UCF-Sports和J-HMDB相比，UCF-101视频的长度更长，并且定位都是空间暂时性的。

评价指标我们在实验中使用了三个度量标准：(1)frame-AP，在帧级别上的平均检测精度，如[8]；(2)video-AP，在视频级别上的平均精度为在[8，9]中。我们将UCF-体育和J-HMDB上的frame-AP和video-AP测量的IOU阈值定为[0.2，0.5]，在UCF101上将其定为[0.05，0.1，0.2，0.3]。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1njz0peldj20nc09r75m.jpg)

`6.2  Implementation details 实施细节`

我们基于Caffe开源工具箱1实现我们的方法。光流利用Brox等人的在线代码进行估计。对于外观和运动R-CNN，我们使用相同的设置，除了运动R-CNN使用128，128，128作为平均值。类似于[36]，我们使用单个样本，即单个图像或者是一张叠加的光流图，在每一次训练迭代中都标注着地面真相盒。当微调VGG-16模型时，我们只更新从凸集1到更高的层，在[36]中观察到是有效的。在行动区域建议网络培训方面，我们将IOU大于0.7的区域设为正区域，IOU小于0.3的区域设置为负区域。对于速度更快的R-CNN的分类部分，我们使用256项提案，其中四分之一作为RPN的正边框，其中，阳性框的IOU大于0.5，负值的IOU为负值。在0.1至0.5之间。在UCF-Sports和J-HMDB上训练两流R-CNN时，将学习速率初始化为10−3，50K迭代后将其降至10−4，70K迭代后停止训练.在训练多区域两流R-cnn时，我们只对TS R-cnn模型的全连通层进行微调，并将学习速率设置为10−4，经过7k迭代后将其更改为10−5,10k之后停止。由于UCF 101数据集是一个大得多的数据集，所以我们在经验上将提到的迭代次数增加了一倍。ROI池层网格固定在7×7。完全连接层的辍学率为在所有情况下都设置为0.5。函数τ(OV)的阈值ψ(OV)按经验固定在0.2。

`6.3 Evaluation of multi-region two-stream faster R-CNN`

在这一部分中，我们首先从四个方面来评价我们的帧级检测方法：RGB/Flow叠加、流组合、多尺度训练和测试以及多区域方案。然后给出了视频级的时空检测和基于检测的动作分类结果。
RGB和光流速度更快的R-CNN有几个帧.


表1：对不同培训和测试量表的评价。所有来自不同尺度的检测都由NMS组合，RGB-1和Flow-5流采用平均分数组合。我们报告UCF-Sports和J-HMDB的结果，aplit 1.
![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nk8hzfbvj20ma03zmxq.jpg)


我们比较外观和移动速度更快的R-CNN与一个或多个框架层检测(平均AP)，见图4。对于这一评估，培训/测试量表被确定为600，相当于t。输入图像的短边。我们可以观察到，在UCF-运动和J-HMDB上，为一个帧(RGB-1)提取的RCNN外观优于为一个帧(Flow-1)提取的运动R-CNN。增增外观模型(RGB-5)的帧数并不能提高性能。然而，使用5帧流可以显着地提高RCNN的性能，即我们可以获得10.27%的效果。在UCF-体育和17.39%的J-HMDB分裂1.这主要是因为来自一个帧的运动信息不够有差别，请参见[22]。在UCF-Sports上堆叠更多的流量(Flow-10)会显着地降低结果，而在J-HMDB上则会稍微减少，部分原因是SEC提到的降低了的建议。3.我们注意到在“跳水”、“跳高杆”、“踢腿”和“跳跃”等动作中，N的表现更为重要。这可以用堆叠不对齐ac的事实来解释。TORS和由此检测到的包围盒是不精确的。总之，Flow-5表现最好，是RGB-1的补充.接下来我们将讨论不同的组合方案。

两条溪流组合。我们探索了两种结合出现和运动的Rcnn方案：盒级非最大抑制(Nms)和与roi融合层(我们的端到端流水线)的得分融合。见图1)。NMS方法分别对外观和运动R-CNN进行检测，然后将两个流中所有检测到的包围盒与NMS融合。如图4所示，对于融合RGB-1和Flow-5流，分数融合(用“我们的”表示)比J-HMDB上的NMS融合提高2.02%，在UCF-Sports上执行PAR。与nms融合相比，sc矿石融合利用外观信息和运动信息进行包围盒评分，具有更强的判断力。在本文的剩余部分中，我们将分数融合用于外观与m的结合。默认情况下，对两流R-CNN使用“RGB-1 Flow-5”.

多规模的培训和测试。一个动作可以在任意的尺度上发生。在这里，我们通过使用多尺度的训练和测试来探讨对尺度变化的鲁棒性.我们将单尺度训练/测试的刻度定为600，并将其修正为{480，600，800}的多尺度情况。结果如表1所示。单尺度测试时，多尺度训练的结果与单尺度训练相当。然而，采用多尺度测试的多尺度训练比传统的多尺度训练有更好的效果。其他设置。特别是，它提高了4%的单一规模的训练和测试的RGB-1R-CNN模型的UCF-体育.我们的两流R-cnn具有多尺度设置，分别获得82.3%和56.6%。UCFSports和J-HMDB分别为%。在论文的剩余部分，我们确定了多尺度训练和测试的设置。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nkd3v8eej20ni0dlacv.jpg)

多区域R-CNN。在多区域评价中，我们采用了两流模型RGB-1流-5和各部分R-CNN模型的多尺度设置.我们报告了我们地区R-CNN在表2中为UCFSports建模，在表3中为J-HMDB.在这两个数据集上的所有R-cnn模型中，Org R-cnn在平均AP方面取得了最好的性能，这表明全身都是es。行动的先见之明。在UCF-Sports上，下半部分和边界模型得到的结果与Org模型相似，而上半部分模型比Org模型差9%。相反，在J-HMDB模型下半模型得到的结果最差，其他区域模型得到的结果与Org模型相似。这反映了两个数据集中的不同类型的操作，即J-HMDB。是由上半身动作(日常动作)所主导，而对于UCF-运动来说，下半身动作是最有特色的(运动动作)。多区域两流R-cnn(MRTS R-cnn)模型I在UCF-Sports和J-HMDB数据集上，Org R-CNN模型分别改进了2.21%和1.6%.它在这两个数据集上都有很大的利润率，超过了最先进的方法[8，9]。

此外，我们观察到个别的R-CNN模型在某些动作上比Org模型表现得更好.例如，上半部分模型的收益 14.1%的“高尔夫”在UCF-体育和8.7%的“摇摆棒球”在J-HMDB的奥格模型。另外，在UCF-Sports上的“Swing 1”和J-HMDB上的“攀爬”模式下半部分的涨幅分别为5.16%和7.9%。无花果Ure 5举例说明了这一点。第(A)和(B)行的动作用上半R-CNN模型更好地检测，而第(C)和(D)行则用下半R-CNN模型来检测。我们可以观察到盒子和它们的分数在不同的模型之间有很大的差异。通过将重点放在底部，“爬楼梯”(d行，第3列)的例子由于dis的存在而获得了较高的置信度检测结果。楼梯和腿的犯罪线索。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nkf02z0jj20nn0kjqqy.jpg)

连接和时间定位。我们评估了两种流R-CNN模型及其多区域版本的连接和时态定位方法。表4显示了视频地图的结果UCFSports、J-HMDB和UCF 101数据集上δ的IOU阈值.我们的两种方法都能在这些数据集上获得优异的视频级性能，这主要是由于高质量的帧级dete。药膏。用我们的连接方法，在UCF-Sports和J-HMDB(拆分1)上分别得到94.82%和70.88%.用[8]中的连接法，相应的数目分别为94.81%和68.97。
结果在J-HMDB上有所改进，但UCF-Sports的检测结果与之相似，后者的检测值接近于完美。

![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nkgz1223j20n20awmz9.jpg)

在J-HMDB和UCF 101上，多区域TS R-CNN的性能一贯优于最初的TS R-CNN模型，在UCF-Sports上的表现也类似.UCF-Sports缺乏改进的原因可能是时空注释中的错误，这就解释了为什么UCF-Sports的分类性能确实有所提高。注意，我们只在UC 101上执行时间定位。尼沃瑟尔大部分动作课几乎涵盖了整个视频。通过MR-TS R-cnn模型的时间定位，我们在视频地图上观察到IOU阈值为0.2的增益为2.0%，但在操作“Basketb”时，增益为2.0%。“所有”、“篮球扣篮”、“天鹅绒”和“伏雷斯派金”，分别提高了23.5%、5.8%、20.0%和8.8%。

通过检测进行分类。类似于[8]，我们的方法也可以推广到视频的行动分类。我们利用最佳动作轨道(即动作得分最大的赛道)预测动作标签的视频。表5报告了UCF-Sports和J-HMDB的平均等级准确性.我们的两种型号都取得了优异的性能，多区域版本改进了t。这两种情况都是他造成的。特别是，我们的MR-TS R-CNN模型在UCF-Sports和J-HMDB上分别获得95.74%和71.08%.结果明显优于IDT方法[2]a基于姿态的CNN方法[21]只进行分类。这表明，通过精确定位和检测感知特征，可以提高分类效果。

`6.4 Comparison to the state of the art`

我们通过与表6中的最新技术进行比较来总结实验评估。在帧级和视频级地图中，我们的TS R-cnn已经超过了所有三个数据集。特别是，我们的MR-TS R-cnn方法在帧地图上的表现比目前先进的分别高出12.6%、12.7%和29.89%，在ucf-Sports、j-hmdb和ucf上的视频地图分别超过了4.3%、12.4%和26.06%。101分别。[8]和[9]也使用了R-CNN的框架式动作检测.
![](http://ww1.sinaimg.cn/large/9dc4c374gy1g1nkkc4h2vj20nh0c1tb5.jpg)
Weinzaephel等人[9]从整个视频中为每个类选择前两个帧级检测，然后跟踪它们基于类级别和实例级别的分数。与[8]和[9]相比，我们的方法受益于两个要点：(1)从外观和运动两方面提出高质量的RPN方案；(2)基于层叠光流和多个部分的区分帧级动作表示。

**7 结论** 
本文介绍了一种多区域两流R-cnn动作检测方法，它充分利用了三种最新的检测方法，即更快的rcn、具有光流叠加的两流cnn和多区域CNN。我们在这些方法的基础上提出了一种新的动作检测框架。它的表现显着超过了最先进的[8，9]。在UCF 101的实验中，我们观察到这一限制在于处理低质量的视频和小的包围盒，这将在今后的工作中解决。

### Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

### Faster R-CNN：基于RPN网络的实时目标检测

**摘要：** 最先进的目标检测网络依赖于区域proposal算法来假设物体的位置。SPPnet和FastR-CNN等先进技术减少了这些检测网络的运行时间，将区域proposal计算暴露为瓶颈。在本工作中，我们引入了一个区域提案网络(RPN)，它与检测网络共享完整的图像卷积特征，从而实现了几乎免费(costfree)的区域提案(region proposal)。RPN是一个完全卷积的网络，它同时预测每个位置的对象边界和对象性分数。RPN是经过端到端的训练，以产生高质量的region proposal，用于Fast R-cnn的检测.我们通过共享RPN和FastR-CNN的卷积特性，进一步将RPN和Fast R-CNN合并成一个单一的网络。通过使用最近流行的“attention”机制的神经网络术语，RPN组件会告诉整体网络向哪里看齐。对于非常深的vgg-16模式，我们的检测系统在GPU上的帧速率为5fps（包括所有步骤），同时在PASCAL VOC 2007,2012和MS COCO数据集上实现了最先进的物体检测精度，并且每张图片只有300个proposals。在ILSVRC和COCO 2015比赛中，Faster R-CNN和RPN是在几个曲目中获得第一名参赛作品的基础。代码已公开提供。

**1 介绍：**最近在目标检测方面的进展是由区域提议方法和基于区域的卷积神经网络的成功所推动的。虽然像[5]中最初开发的那样，基于区域的CNN计算成本很高，但是由于proposal之间共享卷积，它们的成本已经大大降低了。在最新的版本中，如果当忽略在区域proposal上花费的时间时，Fast -R-CNN使用非常深的网络实现了几乎实时的速率。现在，在最先进的检测系统中，proposal是测试时间计算的瓶颈.区域建议方法通常依赖廉价的特征和经济的推理方案。Selective Search，最流行的方法之一，贪婪地合并超像素基于工程低水平的特点.然而，与高效的检测网络[2]相比，选择性搜索速度要慢几个数量级，CPU实现中每幅图像的搜索速度为2秒。EdgeBox[6]目前在提案质量和速度之间提供了最佳的折衷，每幅图像0.2秒。尽管如此，区域建议步骤仍然消耗了与检测网络相同的运行时间。人们可能会注意到，快速基于区域的cnn使用gpu加速，而用于研究的区域建议方法则在cpu上实现。加速提案计算的一个显而易见的方法是为GPU重新实现它。这可能是一个有效的工程解决方案，但重新实现忽略了下游检测网络，因此错过了共享计算的重要机会。



**2 相关工作：**

**3 Faster R-CNN：**

**3.1 RPN：**

**3.1.1 Anchor**

**3.1.2 Loss Function：**

**3.1.3 Training RPNs：**

**3.2 Sharing Feature for RPN and Fast R-CNN：**


**3.3 实施细节：**



**4 实验：**


**4.1**

**5 结论：**





# 实验室工作记录

## 本地环境搭建

本人的机子搭配为Win10➕NVIDIA GeForce GTX 750 Ti 2G显卡搭建TensorFlow深度学习GPU环境。

首先安装conda，然后按照网上搜索的教程安装就ok！

可惜显卡内存太小，跑不动CNN。

**参考文献：**

[Tensorflow 搭建自己的神经网络 (莫烦 Python 教程)](https://www.bilibili.com/video/av16001891)

[windows7系统，NVIDIA GeForce GTX 750 Ti 2G显卡搭建caffe、TensorFlow、Keras深度学习GPU环境](https://blog.csdn.net/zhoushenghuang/article/details/84193950)

## 实验室环境

实验机器是惠普的主机 Xeon 8vCpu + Geforce GTX 1080 Ti

## 第一周

由于装环境时安装的是Win10，没有注意到相关代码是在Linux下开发的，所以在Win10平台上搭建的Caffe环境算是白辛苦一趟，后面还是重装系统并且按照网上搜索到额教程搭建好环境。主要的步骤是：
- 安装Liunx， Ubuntu 16.04
- 去掉Python3，安装Python2，pip等，主要基于相关代码，所以只使用Python2
- 降低系统内核，把Linux系统内核降低至4.4，以便安装CUDA
- 安装显卡驱动
- 安装CUDA，8GA1
- 安装CUDNN， 8
- 编译Faster-R-CNN相关的Caffe
- 跑示例代码

算是跑完了这个搭环境的坑。


主要参考了：

- [Ubuntu16.04 faster-rcnn+caffe+gpu运行环境配置以及解决各种bug](https://blog.csdn.net/flygeda/article/details/78638824)

